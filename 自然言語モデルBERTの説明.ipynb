{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自然言語処理モデルーBERT説明\n",
    "\n",
    "BERT[(Bidirectional Encoder Representations from Transformers)](https://arxiv.org/abs/1810.04805)は google さんが２０１８年１０月に研究開発されました。\n",
    "\n",
    "\n",
    "* BERTのアーキテクチャーについて\n",
    "\n",
    "\n",
    "* BERTの事前学習モデル \n",
    "\n",
    "\n",
    "* ディープラーニングのモデルについて、簡単に説明\n",
    "\n",
    "\n",
    "* BERTのTutorial（文書の感情分析）を検証します。\n",
    "\n",
    "\n",
    "* BERTに関して、その他情報\n",
    "\n",
    "\n",
    "* 自然言語処理の学習リソースをご紹介\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTのアーキテクチャーについて\n",
    "\n",
    "BERT BASE と BERT LARGE\n",
    "\n",
    "<img src=\"bert-base-bert-large-encoders.png\"  width=75% >\n",
    "\n",
    "<img src=\"bert-output-vector.png\" width=100% >\n",
    "\n",
    "inputの部分はテキストから単語の単位で分割してトークン化にして、トークンした数字をモデルにinputします。\n",
    "\n",
    "モデルの部分は事前学習したモデルを利用し、実装します。\n",
    "\n",
    "outputの青い部分から、分類とか数値の予測とかFeed-forwardで結果を出力できます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTの事前学習モデル \n",
    "\n",
    "BERTの事前学習モデル pre-train modelは[huggingface.co](http://huggingface.co)サイトに数多く公表されています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ディープラーニングのモデルについて、簡単に説明\n",
    "\n",
    "\n",
    "<img src=\"neuro_picture.png\"  width=65% >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "サンプルデーターセット｛X,Y｝がありますと。  <span style=\"color:red; font-size:3em;\">f(x)=ax+b</span>  のファンクションの係数<span style=\"color:red; font-size:3em;\">a,b</span>を解くことです。\n",
    "\n",
    "\\begin{equation}\\label{eq:}\n",
    "f(x)=y \\\\ \\left\\{\\ x,y  | \\forall x \\in X, \\forall y \\in Y \\right\\}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "例えば、２つサンプルデーターがありますと、下記計算式の解は逆算できます。aとbのニューロン係数を解きます。。\n",
    "\n",
    "\\begin{cases}\n",
    "    ax_1+b=y_1        & \\left\\{\\ x_1,y_1  | x_1 \\in X,  y_1 \\in Y \\right\\} \\\\\n",
    "    ax_2+b=y_2        & \\left\\{\\ x_2,y_2  |  x_2 \\in X,  y_2 \\in Y \\right\\} \\\\\n",
    "    \\vdots            &  \\vdots   \\\\\n",
    "    ax_n+b=y_n        & \\left\\{\\ x_n,y_n  |  x_n \\in X,  y_n \\in Y \\right\\} \\\\\n",
    "\\end{cases}\n",
    "\n",
    "\n",
    "ニューラルネットワークの訓練は、バイオロジーと同じく実験に失敗してからの経験を重なって、この経験を利用し、係数を修正すると最後に正しくなることです。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTのTutorial（文書の感情分析）を検証します。\n",
    "\n",
    "ちょっと変わったら、顧客意見探知など出来ます。\n",
    "\n",
    "日本語のBERTモデルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('daigo/bert-base-japanese-sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####  Tokenizetion について、、、\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['東京', '都', '、', '19', '日', 'に', '休業', '要請', 'を', '全面', '解除', 'へ']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('東京都、19日に休業要請を全面解除へ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[391, 409, 6, 39, 32, 7, 16212, 4432, 11, 6060, 6140, 118]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('東京都、19日に休業要請を全面解除へ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['意味', 'の', 'ない', 'アラ', '##ート', '発動', 'だっ', 'た', '。']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('意味のないアラート発動だった。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[967, 5, 80, 2416, 145, 9144, 308, 10, 8]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokenizer.tokenize('意味のないアラート発動だった。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data について"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特別なトークン：[PAD] , [UNK] , [CLS] , [SEP] , [MASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 967, 5, 80, 2416, 145, 9144, 308, 10, 8, 3]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('意味のないアラート発動だった。', add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "odict_keys(['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', 'の', '、', 'に', '。', 'は', 'た', 'を', 'で', 'と', 'が', 'し', 'て', '1', 'な', '年', 'れ', 'い', 'あ', '(', ')', '2', 'さ', 'こ', 'も', 'か', '##する', 'ある', '日', 'いる', 'する', '・', '「', '月', '」', '19', 'から', '20', '大', 'ア', 'そ', 'こと', '##して', 'ま', '3', 'や', 'として', '中', '一', '人', 'よ', 'ス', 'によ', '4', 'なっ', 'その', 'ら', '-', 'れる', '『', 'など', '』', 'フ', 'シ', '##リー', '同', 'この', '出', '時', 'お', '地', 'だ', '5', '行', '201', '国', 'ない', '的', 'ため', '後', 'られ', '発', '200', '##ール', 'イ', '##ラン', '作', '日本', '##ター', '##ック', '市', '戦', 'マ', '第', '自', 'コ', '以', '6', 'あっ', 'カ', '者', '開', 'また', '高', '本', '上', 'オ', '##ット', '学', '最', 'とい', '現', 'バ', 'サ', 'へ', '##スト', 'もの', '10', '東', '##ング', 'よう', '名', 'まで', '7', '生', '部', 'あり', '世', 'プ', 'エ', '8', '##ュー', '会', '主', 'ロ', 'なる', 'という', '多', '\"', '.', '機', '##ート', '事', '新', 'デ', '県', 'ジ', '全', '##いて', '##ーム', '##って', '分', '選', 'ク', '##ティ', '9', '小', 'ド', '当', '18', '##ント', 'ト', 'ラ', '家', 'メ', '##ード', '公', 'これ', 'レ', '場', '前', 'チ', '初', '長', 'パ', '通', 'S', 'つ', 'リ', '入', 'C', 'キ', '内', '##かっ', '受', '特', 'にお', 'せ', 'A', '実', '##ンド', '連', '使', '12', '回', '軍', '##ョン', '代', '199', 'でき', '記', 'う', 'おり', '教', '設', 'ブ', '北', '##ディ', '見', '11', '文', 'テ', '対', '##イン', 'それ', '目', 'ハ', 'より', ')、', '平', '結', 'により', '経', 'タ', ',', '##リア', 'によって', '##00', 'ほ', '所', '関', '駅', 'M', '化', '##ース', '運', '三', '用', '##リカ', 'ウ', '放', '性', '番', '位', '政', 'モ', 'による', '下', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTのニューラルネットワークを作成、"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"daigo/bert-base-japanese-sentiment\",\n",
    "    num_labels = 2,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  8415, 28457, 28457,     3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"気持ちいい\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "#labels = torch.tensor([1]).unsqueeze(0)  # Batch size 1\n",
    "#outputs = model(input_ids, labels=labels)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 1.9109, -2.1092]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax function で分類する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9823653921066929"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1.9109)/(np.exp(1.9109)+np.exp(-2.1092))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor(tokenizer.encode(\"気持ち悪い\", add_special_tokens=True)).unsqueeze(0)  # Batch size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-2.2444,  1.9429]], grad_fn=<AddmmBackward>),)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9850399664465486"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(1.9429)/(np.exp(-2.2444)+np.exp(1.9429))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "評価ファンクション"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sa_function(text):\n",
    "    input_ids = torch.tensor(tokenizer.encode(text, add_special_tokens=True)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    positive = outputs[0][0][0].item()\n",
    "    negative = outputs[0][0][1].item()\n",
    "    positive_p = np.exp(positive)/(np.exp(positive)+np.exp(negative))\n",
    "    negative_p = np.exp(negative)/(np.exp(positive)+np.exp(negative))\n",
    "    if positive_p>=negative_p:\n",
    "        result = {\"label\" : \"ポジティブ\",\"score\" : positive_p}\n",
    "    else:\n",
    "        result = {\"label\" : \"ネガティブ\",\"score\" : negative_p}\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデル分類の効果を試します。\n",
    "\n",
    "アマゾンサイトでコメント　と　ヤフーニュースのコメントで、軽く試しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'ネガティブ', 'score': 0.7423163061031355}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sa_function('capslockキーが使えなくなりました。ここまでひどいキーボードは初めてです。')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'ネガティブ', 'score': 0.8131462307951975}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sa_function('絶対に嫌だ。気持ち悪い旦那とこれからも一緒にいようだなんて。子供もいるし旦那が収入あろうとしても複数の女とやって天狗の渡部は嫌だな')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'label': 'ネガティブ', 'score': 0.7500206617469946}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa_function('むしろ、コロナを知らない？名前が同じだからとああだこうだと言ってるバカ達は歴史も知らないのかね。バカは困る、ホントに。')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERTの一族に関して、その他情報\n",
    "\n",
    "その他transformerのモデル：\n",
    "\n",
    "   * ELMO(94M); BERT(340M); GPT-2(1542M); Megatron(8G); T5(11G); Turing NLG(17G) (weight numbers)係数の量\n",
    "\n",
    "普通のGPUだったら、BERTギリギリを使える感じです。これ以上はほぼ無理です。しかもモデル性能が高い反面、悪用の懸念があるため、まだ事前学習モデルが未公開多いです。\n",
    "\n",
    "\n",
    "Open AiさんのGPT-2モデルに関して、事前学習モデル（公表されていません。）の性能\n",
    "\n",
    "\n",
    "https://talktotransformer.com/\n",
    "\n",
    "ALBERT:\n",
    "\n",
    "   * BERTの軽量モデル。ｰ>性能が落ちなく、モバイルなどリソースが限られる利用のケースです。\n",
    "\n",
    "最近に発表したGPT-3モデル：　\n",
    "\n",
    "   * 最も大きい性能が人間より良いモデルです。パラメーラー数も大きいです。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然言語処理のコースをご紹介\n",
    "\n",
    "また、学生さんや、技術者、NLPのバックグランドを知りたい人に\n",
    "\n",
    "もっと細かく自然言語処理を勉強したいだったら、下記のコースは私のレコメンドです。\n",
    "\n",
    "* スタンフォード大学2017年秋のNLPコース：\n",
    "https://www.youtube.com/playlist?list=PL3FW7Lu3i5Jsnh1rnUwq_TcylNr7EkRe6\n",
    "\n",
    "* Coursera:\n",
    "https://www.coursera.org/learn/nlp-sequence-models\n",
    "\n",
    "２つコースは最新のBERT技術は含めてないですが。どちらでも大事な基礎の部分です。なければBERTの理解はできません。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "次回の内容紹介：\n",
    "\n",
    "訓練データーセットのデーター前処理\n",
    "\n",
    "ちょっとどうやってモデルを訓練しますかを説明します。\n",
    "\n",
    "RTE(Recognizing Textual Entailment) -> 推論認識でのBERTモデルの紹介\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "262.05px",
    "left": "592.683px",
    "top": "111.333px",
    "width": "410.719px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
